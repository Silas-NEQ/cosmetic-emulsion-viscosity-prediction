# Final Technical Report: Cosmetic Emulsion Viscosity Prediction Project

## ðŸ’¡ Methodological Note
**Important**: This project uses a **synthetic dataset** generated by artificial intelligence (AI) to simulate cosmetic emulsion production data. The primary objective is to demonstrate the practical application of machine learning algorithms in an industrial context, serving as a technical portfolio piece and educational case study.

## 1. Executive Summary
This project developed a predictive system to estimate the final viscosity of a cosmetic emulsion during the production process. Five machine learning algorithms were implemented and compared, with Polynomial Regression selected as the final model due to its ideal balance between predictive performance and computational efficiency.

## 2. Introduction

### 2.1 Industrial Context
Viscosity represents one of the most critical properties in cosmetic emulsion formulations, directly influencing multiple aspects of the final product:

- **Physicochemical stability**: Inadequate viscosity values can accelerate phase separation phenomena such as creaming and sedimentation (TADROS, 2013)
- **Sensory characteristics**: Viscosity determines parameters like spreadability, cutaneous absorption, and tactile sensation, crucial factors for consumer acceptance (SCHRAMM, 2005)
- **Industrial processability**: During manufacturing, viscosity directly affects homogenisation efficiency, pumping, and filling operations (MYERS, 2005)

### 2.2 Business Problem
Variations in process parameters (temperature, agitation, emulsifier concentration) frequently result in inconsistencies in final viscosity, leading to:

- Rework of out-of-specification batches
- Waste of raw materials and inputs
- Inconsistency in final product quality
- Negative impact on production efficiency

The ability to predict final viscosity based on monitorable process variables would enable proactive interventions, optimising quality control and reducing operational losses.

## 3. Methodology

### 3.1 Data Collection and Preparation
- **Source**: Synthetic dataset generated by AI to replicate real production patterns
- **Simulated variables**: Typical industrial process parameters including temperature (Â°C), agitation speed (RPM), concentration (%), reaction time (min), pH, humidity (%) and viscosity (cP)
- **Pedagogical objective**: Develop and validate machine learning methodologies applicable to industrial contexts

### 3.2 Pre-processing
- Encoding of categorical variables
- Normalisation of numerical features
- Stratified split: 80% training, 20% testing

### 3.3 Implemented Models

**Polynomial Regression**
- **Complexity**: Degree 2

**Artificial Neural Networks**
- **Parameters**: `activation='relu'`, `alpha=0.01`, `hidden_layer_sizes=(50,)`, `learning_rate_init=0.001`, `solver='lbfgs'`, `max_iter=2000`

**XGBoost**
- **Parameters**: `n_estimators=300`, `max_depth=3`, `learning_rate=0.05`

**Ensemble**
- **Composition**: Polynomial Regression, Neural Networks and XGBoost
- **Method**: Weighted average of predictions

**Random Forest**
- **Parameters**: `n_estimators=500`, `max_depth=20`, `max_features=0.5`, `min_samples_leaf=1`, `min_samples_split=2`

**SVM**
- **Parameters**: Default values. No deeper analyses were conducted as the algorithm obtained a very low RÂ² value (0.07) in the initial analysis and was immediately discarded

## 4. Results and Analysis

### 4.1 Comparative Performance

| Model | RÂ² Score | MAE | Performance Ranking |
|-------|----------|-----|---------------------|
| Polynomial Regression | 0.903 | 242.61 | ðŸ¥‡ **1st Place** |
| Ensemble | 0.899 | 246.98 | ðŸ¥ˆ 2nd Place |
| Neural Network | 0.891 | 253.26 | ðŸ¥‰ 3rd Place |
| XGBoost | 0.885 | 263.96 | 4th Place |
| Random Forest | 0.868 | 273.80 | - |
| SVM | 0.07 | 759.49 | - |

### 4.2 Detailed Analysis

#### 4.2.1 Polynomial Regression (Selected Model)
- **RÂ² = 0.903**: Explains 90.3% of the variance in viscosity
- **MAE = 242.61**: Acceptable mean error for the dataset
- **Processing Speed**: Execution time of 0.08s/run

#### 4.2.2 Performance vs. Efficiency
- **Ensemble**: Slightly inferior performance (RÂ² = 0.899) with significantly higher computational cost, due to being a composition of the 3 final models
- **Neural Network**: Similar performance to ensemble (RÂ² = 0.891) with the highest processing time among the final models (27.66s/run). Unnecessary complexity
- **XGBoost**: Inferior performance among the 3 final models (RÂ² = 0.885) in this specific application, but second best processing time (1.17s/run)
- **Random Forest**: Model discarded after cross-validation process due to poorest performance. Showed the lowest RÂ² value (RÂ² = 0.868) and highest MAE value (MAE = 273.80), in addition to the longest execution time (38.41s/run)

## 5. Conclusion

- The 3 final models demonstrated relevant predictive capability (RÂ² > 0.85)
- The non-linear relationship between process variables and viscosity was adequately captured
- The simplicity of Polynomial Regression proved advantageous compared to more complex models

**Final Model Decision**
Polynomial Regression was selected as the definitive model due to:
- âœ… Leading performance (RÂ² = 0.903)
- âœ… Superior computational efficiency (1.17 s/run)
- âœ… Ease of implementation and maintenance
- âœ… Best operational cost-benefit ratio

## 6. Technical References
1. **TADROS, T. F.** *Emulsion Science and Technology: A General Introduction*. Wiley-VCH, 2013.
2. **SCHRAMM, L. L.** *Emulsions, Foams, and Suspensions: Fundamentals and Applications*. Wiley-VCH, 2005.
3. **MYERS, D.** *Surfactant Science and Technology*. 3rd ed. Wiley, 2005.
4. **HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J.** *The Elements of Statistical Learning*. 2nd ed. Springer, 2009.
5. **GÃ‰RON, A.** *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*. 2nd ed. O'Reilly, 2019.
6. Technical documentation: Scikit-Learn, XGBoost, 2023.

---

**Report Date**: 28th October 2023  
**Prepared by**: Silas Nascimento  
**Area**: Chemical Engineering / Artificial Inteligence